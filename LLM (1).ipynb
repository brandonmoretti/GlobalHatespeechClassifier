{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d38662c-52ff-4c56-a99a-8502bc8c7c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor start_idx in range(0, len(texts_list), chunk_size):\\n    process_and_save_chunk(start_idx)\\n\\nfinal_df = pd.DataFrame({\\'text\\': texts_list, \\'gpt4_target\\': gpt4_predictions})\\nfinal_df.to_csv(\\'gpt4_predictions_final.csv\\', index=False)\\n\\nimport matplotlib.pyplot as plt\\n\\ndf = pd.read_csv(\"gpt4_predictions_final.csv\")\\n\\ntarget_counts = df[\\'gpt4_target\\'].value_counts().sort_values(ascending=False)\\n\\nplt.figure(figsize=(14, 6))\\ntarget_counts.plot(kind=\\'bar\\', color=\\'teal\\')\\nplt.title(\"GPT-4 Predicted Hate Speech Targets (Hateful Texts Only)\")\\nplt.xlabel(\"Target Identity\")\\nplt.ylabel(\"Count\")\\nplt.xticks(rotation=45, ha=\\'right\\')\\nplt.tight_layout()\\nplt.show()\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib\n",
    "from datasets import load_dataset\n",
    "from time import sleep\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "#IMDB_DATASET = load_dataset(\"imdb\", split='train').shuffle(42)[0:200]\n",
    "#IMDB_DATASET_X = IMDB_DATASET['text']\n",
    "#IMDB_DATASET_Y = IMDB_DATASET['label']\n",
    "#del IMDB_DATASET  \n",
    "\n",
    "OPENAI_API_KEY = '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'\n",
    "cache = {}\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def run_gpt4(prompt, return_first_line=True):\n",
    "    \"\"\"\n",
    "    Run GPT-4 model to generate a prediction based on a prompt.\n",
    "    Implements retries in case of failure.\n",
    "    \"\"\"\n",
    "    cache_key = (prompt, return_first_line)\n",
    "    if cache_key in cache:\n",
    "        return cache[cache_key]\n",
    "\n",
    "    response_text = None\n",
    "\n",
    "    for i in range(0, 60, 6):\n",
    "        try:\n",
    "            response = client.responses.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                input = prompt,\n",
    "                temperature=0,\n",
    "                #max_tokens=100,\n",
    "                #top_p=1,\n",
    "                #frequency_penalty=0.0,\n",
    "                #presence_penalty=0.0,\n",
    "            )\n",
    "            response_text = response.output_text\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            sleep(i)\n",
    "\n",
    "    if response_text is None:\n",
    "        final_response = \"ERROR: No response received from model.\"\n",
    "    else:\n",
    "        if return_first_line:\n",
    "            final_response = response_text.split('.')[0] + '.'\n",
    "            if '\\n' in final_response:\n",
    "                final_response = final_response.split('\\n')[0]\n",
    "        else:\n",
    "            final_response = response_text\n",
    "\n",
    "    cache[cache_key] = final_response\n",
    "    return final_response\n",
    "\n",
    "#berkeley = pd.read_parquet(\"hf://datasets/ucberkeley-dlab/measuring-hate-speech/measuring-hate-speech.parquet\")\n",
    "\n",
    "'''columns_relevant =  ['hatespeech', 'text', 'target_race_asian', 'target_race_black', 'target_race_latinx', 'target_race_middle_eastern',\n",
    "'target_race_native_american', 'target_race_pacific_islander', 'target_race_white', 'target_religion_atheist', 'target_religion_buddhist',\n",
    "'target_religion_christian', 'target_religion_hindu', 'target_religion_jewish', 'target_religion_mormon', 'target_religion_muslim', \n",
    "'target_religion_other', 'target_origin_immigrant', 'target_origin_migrant_worker', 'target_origin_undocumented', \n",
    "'target_gender_men', 'target_gender_non_binary', 'target_gender_transgender_men', 'target_gender_transgender_unspecified',\n",
    "'target_gender_transgender_women', 'target_gender_women', 'target_sexuality_bisexual', 'target_sexuality_gay', 'target_sexuality_lesbian',\n",
    "'target_sexuality_straight', 'target_sexuality_other', 'target_disability_physical', 'target_disability_cognitive',\n",
    "'target_disability_neurological', 'target_disability_visually_impaired', 'target_disability_hearing_impaired', 'target_disability_unspecific',\n",
    "'target_disability_other']\n",
    "\n",
    "hateful_berkeley = berkeley[berkeley['hatespeech'] == 1][columns_relevant]\n",
    "\n",
    "texts_list = hateful_berkeley.sample(n=100, random_state=42)['text'].reset_index(drop=True)\n",
    "''' \n",
    "'''\n",
    "\n",
    "\n",
    "chunk_size = 50\n",
    "\n",
    "output_dir = 'gpt4_predictions_intermediate'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "gpt4_predictions = []\n",
    "\n",
    "QA_PROMPT = \"\"\"\n",
    "Given the following text, identify the target of the hate speech.\n",
    "Here is a list of possible target identities:\n",
    "    Race: asian, black, latinx, Middle Eastern, Native American, Pacific Islander, White;\n",
    "    Religion: Atheist, Buddhist, Christian, Hindu, Jewish, Mormon, Muslim, other;\n",
    "    Origin: Immigrant, Migrant worker, undocumented;\n",
    "    Gender: Non-Binary, Transgender-Man, Transgender-Woman, Transgender-unspecified;\n",
    "    Sexuality: Bisexual, Gay, Lesbian, other;\n",
    "    Disability: Physical, Cognitive, Neurological, Visually Impaired, Hearing Impaired, Unspecific, other.\n",
    "\n",
    "Text: {input}\n",
    "\"\"\"\n",
    "'''\n",
    "'''\n",
    "def process_and_save_chunk(chunk_start_idx):\n",
    "    chunk_texts = texts_list[chunk_start_idx: chunk_start_idx + chunk_size]\n",
    "    chunk_predictions = []\n",
    "\n",
    "    for text in tqdm(chunk_texts):\n",
    "        try:\n",
    "            prediction = run_gpt4(QA_PROMPT.replace(\"{input}\", text))\n",
    "        except Exception as e:\n",
    "            prediction = \"ERROR\"\n",
    "        chunk_predictions.append(prediction)\n",
    "\n",
    "    chunk_df = pd.DataFrame({'text': chunk_texts, 'gpt4_target': chunk_predictions})\n",
    "    chunk_file_path = os.path.join(output_dir, f\"gpt4_predictions_chunk_{chunk_start_idx // chunk_size}.csv\")\n",
    "    chunk_df.to_csv(chunk_file_path, index=False)\n",
    "\n",
    "    gpt4_predictions.extend(chunk_predictions)\n",
    "\n",
    "'''\n",
    "'''\n",
    "for start_idx in range(0, len(texts_list), chunk_size):\n",
    "    process_and_save_chunk(start_idx)\n",
    "\n",
    "final_df = pd.DataFrame({'text': texts_list, 'gpt4_target': gpt4_predictions})\n",
    "final_df.to_csv('gpt4_predictions_final.csv', index=False)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"gpt4_predictions_final.csv\")\n",
    "\n",
    "target_counts = df['gpt4_target'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "target_counts.plot(kind='bar', color='teal')\n",
    "plt.title(\"GPT-4 Predicted Hate Speech Targets (Hateful Texts Only)\")\n",
    "plt.xlabel(\"Target Identity\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e3427cd-6cb4-4c9b-9aca-98d1b3f89def",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunk_size = 100\n",
    "\n",
    "output_dir = 'gpt4_arabic_predictions_english'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "gpt4_predictions = []\n",
    "\n",
    "QA_PROMPT = \"\"\"\n",
    "The following text is hate speech, identify the target from the following set of labels, answer in only one or two words.\n",
    "Here is a list of possible target identities:\n",
    "    Race: asian, black, latinx, Middle Eastern, White;\n",
    "    Religion:Christian, Jewish, Muslim;\n",
    "    Origin: Immigrant;\n",
    "    Gender: Transgender;\n",
    "    Sexuality: Bisexual, Gay, Lesbian, other;\n",
    "    Disability: Disabled.\n",
    "\n",
    "Text: {input}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2795c0a3-6294-4d07-b7d0-648e3ec50d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def process_and_save_chunk(chunk_start_idx, arabic_list, chunk_size):\n",
    "    chunk_texts = arabic_list[chunk_start_idx: chunk_start_idx + chunk_size]\n",
    "    chunk_predictions = []\n",
    "\n",
    "    for text in tqdm(chunk_texts):\n",
    "        try:\n",
    "            prediction = run_gpt4(QA_PROMPT.replace(\"{input}\", text))\n",
    "        except Exception as e:\n",
    "            prediction = \"ERROR\"\n",
    "        chunk_predictions.append(prediction)\n",
    "\n",
    "    chunk_df = pd.DataFrame({'text': chunk_texts, 'gpt4_target': chunk_predictions})\n",
    "    chunk_file_path = os.path.join(output_dir, f\"gpt4_predictions_chunk_{chunk_start_idx // chunk_size}.csv\")\n",
    "    chunk_df.to_csv(chunk_file_path, index=False)\n",
    "\n",
    "    gpt4_predictions.extend(chunk_predictions)\n",
    "    #gpt4_predictions= chunk_predictions'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ffd5576-1c50-4c8d-90b2-be1c921dfcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I play a trumpet like no other</td>\n",
       "      <td>LGB+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You remain an animal and it takes you a millio...</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>By God, I don’t know what you are laughing at,...</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The dogs of Lebanon are more despicable than t...</td>\n",
       "      <td>Middle Eastern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not one of the Arab donkeys oppressing power b...</td>\n",
       "      <td>Middle Eastern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Yes, Abu Sawawikh, my neighbor is trampling you</td>\n",
       "      <td>Muslim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>I swear to God, my friend, I have acne on my v...</td>\n",
       "      <td>LGB+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Because you are a donkey</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>May God curse you and curse those who curse you</td>\n",
       "      <td>Christian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>You won't be able to sleep except for General ...</td>\n",
       "      <td>Middle Eastern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              english          labels\n",
       "0                      I play a trumpet like no other            LGB+\n",
       "1   You remain an animal and it takes you a millio...           Black\n",
       "2   By God, I don’t know what you are laughing at,...          Muslim\n",
       "3   The dogs of Lebanon are more despicable than t...  Middle Eastern\n",
       "4   Not one of the Arab donkeys oppressing power b...  Middle Eastern\n",
       "..                                                ...             ...\n",
       "95    Yes, Abu Sawawikh, my neighbor is trampling you          Muslim\n",
       "96  I swear to God, my friend, I have acne on my v...            LGB+\n",
       "97                           Because you are a donkey           Black\n",
       "98    May God curse you and curse those who curse you       Christian\n",
       "99  You won't be able to sleep except for General ...  Middle Eastern\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_data_english = pd.read_csv('arabic_model_sample_actual.csv',sep = \",\")\n",
    "#normal = arabic_data_with_normal['Class'] == 'normal'\n",
    "#arabic_data = arabic_data_with_normal[~normal]\n",
    "\n",
    "arabic_data_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27a69b4a-4439-4496-bc85-39899061c5c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                        I play a trumpet like no other\n",
       "1     You remain an animal and it takes you a millio...\n",
       "2     By God, I don’t know what you are laughing at,...\n",
       "3     The dogs of Lebanon are more despicable than t...\n",
       "4     Not one of the Arab donkeys oppressing power b...\n",
       "                            ...                        \n",
       "95      Yes, Abu Sawawikh, my neighbor is trampling you\n",
       "96    I swear to God, my friend, I have acne on my v...\n",
       "97                             Because you are a donkey\n",
       "98      May God curse you and curse those who curse you\n",
       "99    You won't be able to sleep except for General ...\n",
       "Name: english, Length: 100, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_list = arabic_data_english['english'].reset_index(drop=True)\n",
    "arabic_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eed37d4b-95a6-4d90-9ef3-a9725e3dedc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 437818.79it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chunk_texts = arabic_list\n",
    "chunk_predictions = []\n",
    "\n",
    "for text in tqdm(chunk_texts):\n",
    "    try:\n",
    "        prediction = run_gpt4(QA_PROMPT.replace(\"{input}\", text))\n",
    "    except Exception as e:\n",
    "        prediction = \"ERROR\"\n",
    "    chunk_predictions.append(prediction)\n",
    "\n",
    "chunk_df = pd.DataFrame({'text': chunk_texts, 'gpt4_target': chunk_predictions})\n",
    "chunk_df.to_csv(\"arabic_llm_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02b547-c8eb-4de1-bb4c-7f9e510acd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'gpt4_korean_predictions_english'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "gpt4_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49ed4dc-2da4-4ec0-8ab9-a34667e44ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_data_english = pd.read_csv('korean_model_sample_actual.csv',sep = \",\")\n",
    "#normal = arabic_data_with_normal['Class'] == 'normal'\n",
    "#arabic_data = arabic_data_with_normal[~normal]\n",
    "\n",
    "korean_data_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d19f514-ac8f-44de-be5c-a6acac9fcb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: openai: command not found\n"
     ]
    }
   ],
   "source": [
    "korean_list = korean_data_english['english'].reset_index(drop=True)\n",
    "korean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcdf8cc1-72d6-4ca0-9459-8400d5683fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (1.73.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from openai) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from openai) (2.10.6)\n",
      "Requirement already satisfied: sniffio in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
     ]
    }
   ],
   "source": [
    "process_and_save_chunk(0, korean_list, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc61a638-11eb-477d-9229-e7d660ac3e92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
