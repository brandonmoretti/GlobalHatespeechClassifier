{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Downloading absl_py-2.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from tensorflow) (25.2.10)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Downloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Downloading termcolor-3.0.1-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from tensorflow) (1.17.2)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Downloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Downloading keras-3.9.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from tensorflow) (1.26.3)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Downloading namex-0.0.9-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /ihome/cs1671_2025s/seg135/.local/lib/python3.11/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from rich->keras>=3.5.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.2.2-py3-none-any.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m412.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m839.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.71.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.9.2-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.0.1-py3-none-any.whl (7.2 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.9-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (410 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.4/410.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: namex, libclang, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.30.2\n",
      "    Uninstalling protobuf-6.30.2:\n",
      "      Successfully uninstalled protobuf-6.30.2\n",
      "\u001b[33m  WARNING: The script tensorboard is installed in '/ihome/cs1671_2025s/seg135/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The scripts import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert and toco are installed in '/ihome/cs1671_2025s/seg135/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed absl-py-2.2.2 astunparse-1.6.3 gast-0.6.0 google-pasta-0.2.0 grpcio-1.71.0 h5py-3.13.0 keras-3.9.2 libclang-18.1.1 ml-dtypes-0.5.1 namex-0.0.9 opt-einsum-3.4.0 optree-0.15.0 protobuf-5.29.4 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Berkeley Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install evaluate\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib \n",
    "from transformers import DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>text</th>\n",
       "      <th>target_race_asian</th>\n",
       "      <th>target_race_black</th>\n",
       "      <th>target_race_latinx</th>\n",
       "      <th>target_race_middle_eastern</th>\n",
       "      <th>target_race_native_american</th>\n",
       "      <th>target_race_pacific_islander</th>\n",
       "      <th>target_race_white</th>\n",
       "      <th>target_religion_atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>target_sexuality_lesbian</th>\n",
       "      <th>target_sexuality_straight</th>\n",
       "      <th>target_sexuality_other</th>\n",
       "      <th>target_disability_physical</th>\n",
       "      <th>target_disability_cognitive</th>\n",
       "      <th>target_disability_neurological</th>\n",
       "      <th>target_disability_visually_impaired</th>\n",
       "      <th>target_disability_hearing_impaired</th>\n",
       "      <th>target_disability_unspecific</th>\n",
       "      <th>target_disability_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Question: These 4 broads who criticize America...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>It is about time for all illegals to go back t...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hatespeech                                               text  \\\n",
       "0         0.0  Yes indeed. She sort of reminds me of the elde...   \n",
       "1         0.0  The trans women reading this tweet right now i...   \n",
       "2         2.0  Question: These 4 broads who criticize America...   \n",
       "3         0.0  It is about time for all illegals to go back t...   \n",
       "4         2.0  For starters bend over the one in pink and kic...   \n",
       "\n",
       "   target_race_asian  target_race_black  target_race_latinx  \\\n",
       "0               True               True                True   \n",
       "1              False              False               False   \n",
       "2              False              False               False   \n",
       "3              False              False               False   \n",
       "4              False              False               False   \n",
       "\n",
       "   target_race_middle_eastern  target_race_native_american  \\\n",
       "0                        True                         True   \n",
       "1                       False                        False   \n",
       "2                       False                        False   \n",
       "3                       False                        False   \n",
       "4                       False                        False   \n",
       "\n",
       "   target_race_pacific_islander  target_race_white  target_religion_atheist  \\\n",
       "0                          True               True                    False   \n",
       "1                         False              False                    False   \n",
       "2                         False              False                    False   \n",
       "3                         False              False                    False   \n",
       "4                         False              False                    False   \n",
       "\n",
       "   ...  target_sexuality_lesbian  target_sexuality_straight  \\\n",
       "0  ...                     False                      False   \n",
       "1  ...                     False                      False   \n",
       "2  ...                     False                      False   \n",
       "3  ...                     False                      False   \n",
       "4  ...                     False                      False   \n",
       "\n",
       "   target_sexuality_other  target_disability_physical  \\\n",
       "0                   False                       False   \n",
       "1                   False                       False   \n",
       "2                   False                       False   \n",
       "3                   False                       False   \n",
       "4                   False                       False   \n",
       "\n",
       "   target_disability_cognitive  target_disability_neurological  \\\n",
       "0                        False                           False   \n",
       "1                        False                           False   \n",
       "2                        False                           False   \n",
       "3                        False                           False   \n",
       "4                        False                           False   \n",
       "\n",
       "   target_disability_visually_impaired  target_disability_hearing_impaired  \\\n",
       "0                                False                               False   \n",
       "1                                False                               False   \n",
       "2                                False                               False   \n",
       "3                                False                               False   \n",
       "4                                False                               False   \n",
       "\n",
       "   target_disability_unspecific  target_disability_other  \n",
       "0                         False                    False  \n",
       "1                         False                    False  \n",
       "2                         False                    False  \n",
       "3                         False                    False  \n",
       "4                         False                    False  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "berkeley = pd.read_parquet(\"hf://datasets/ucberkeley-dlab/measuring-hate-speech/measuring-hate-speech.parquet\")\n",
    "\n",
    "columns_relevant =  ['hatespeech', 'text', 'target_race_asian', 'target_race_black', 'target_race_latinx', 'target_race_middle_eastern',\n",
    "'target_race_native_american', 'target_race_pacific_islander', 'target_race_white', 'target_religion_atheist', 'target_religion_buddhist',\n",
    "'target_religion_christian', 'target_religion_hindu', 'target_religion_jewish', 'target_religion_mormon', 'target_religion_muslim', \n",
    "'target_religion_other', 'target_origin_immigrant', 'target_origin_migrant_worker', 'target_origin_undocumented', \n",
    "'target_gender_men', 'target_gender_non_binary', 'target_gender_transgender_men', 'target_gender_transgender_unspecified',\n",
    "'target_gender_transgender_women', 'target_gender_women', 'target_sexuality_bisexual', 'target_sexuality_gay', 'target_sexuality_lesbian',\n",
    "'target_sexuality_straight', 'target_sexuality_other', 'target_disability_physical', 'target_disability_cognitive',\n",
    "'target_disability_neurological', 'target_disability_visually_impaired', 'target_disability_hearing_impaired', 'target_disability_unspecific',\n",
    "'target_disability_other']\n",
    "\n",
    "berkeley_columns_relevant = berkeley[columns_relevant]\n",
    "berkeley_columns_relevant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm-14064/ipykernel_76594/3703324346.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  berkeley_columns_relevant.loc[:, 'LGBTQ+'] = berkeley_columns_relevant[lgbtq_cols].any(axis=1)\n",
      "/scratch/slurm-14064/ipykernel_76594/3703324346.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  berkeley_columns_relevant.loc[:, 'Disabled'] = berkeley_columns_relevant[disability_cols].any(axis=1)\n",
      "/scratch/slurm-14064/ipykernel_76594/3703324346.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  berkeley_columns_relevant.loc[:, 'Immigrant'] = berkeley_columns_relevant[immigrant_cols].any(axis=1)\n",
      "/scratch/slurm-14064/ipykernel_76594/3703324346.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  berkeley_columns_relevant.loc[:, 'Christianity'] = berkeley_columns_relevant[christianity_cols].any(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>text</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Black</th>\n",
       "      <th>Latinx</th>\n",
       "      <th>Middle Eastern</th>\n",
       "      <th>Native American</th>\n",
       "      <th>Pacific Islander</th>\n",
       "      <th>White</th>\n",
       "      <th>Atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>Jewish</th>\n",
       "      <th>Muslim</th>\n",
       "      <th>Other Religion</th>\n",
       "      <th>Men</th>\n",
       "      <th>Non-Binary</th>\n",
       "      <th>Women</th>\n",
       "      <th>LGBTQ+</th>\n",
       "      <th>Disabled</th>\n",
       "      <th>Immigrant</th>\n",
       "      <th>Christianity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Question: These 4 broads who criticize America...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>It is about time for all illegals to go back t...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hatespeech                                               text  Asian  \\\n",
       "0           0  Yes indeed. She sort of reminds me of the elde...   True   \n",
       "1           0  The trans women reading this tweet right now i...  False   \n",
       "2           1  Question: These 4 broads who criticize America...  False   \n",
       "3           0  It is about time for all illegals to go back t...  False   \n",
       "4           1  For starters bend over the one in pink and kic...  False   \n",
       "\n",
       "   Black  Latinx  Middle Eastern  Native American  Pacific Islander  White  \\\n",
       "0   True    True            True             True              True   True   \n",
       "1  False   False           False            False             False  False   \n",
       "2  False   False           False            False             False  False   \n",
       "3  False   False           False            False             False  False   \n",
       "4  False   False           False            False             False  False   \n",
       "\n",
       "   Atheist  ...  Jewish  Muslim  Other Religion    Men  Non-Binary  Women  \\\n",
       "0    False  ...   False   False           False  False       False  False   \n",
       "1    False  ...   False   False           False  False       False  False   \n",
       "2    False  ...   False   False           False  False       False  False   \n",
       "3    False  ...   False   False           False  False       False  False   \n",
       "4    False  ...   False   False           False  False       False   True   \n",
       "\n",
       "   LGBTQ+  Disabled  Immigrant  Christianity  \n",
       "0   False     False      False         False  \n",
       "1    True     False      False         False  \n",
       "2   False     False       True         False  \n",
       "3   False     False       True         False  \n",
       "4   False     False      False         False  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_cols = [ 'target_sexuality_bisexual', 'target_sexuality_gay', 'target_sexuality_lesbian', 'target_sexuality_other']\n",
    "\n",
    "t_cols = ['target_gender_transgender_men', 'target_gender_transgender_unspecified','target_gender_transgender_women']\n",
    "\n",
    "disability_cols = ['target_disability_physical', 'target_disability_cognitive', 'target_disability_neurological',\n",
    "'target_disability_visually_impaired', 'target_disability_hearing_impaired', 'target_disability_unspecific', 'target_disability_other']\n",
    "\n",
    "immigrant_cols = ['target_origin_immigrant', 'target_origin_migrant_worker', 'target_origin_undocumented']\n",
    "\n",
    "\n",
    "berkeley_columns_relevant.loc[:, 'Transgender'] = berkeley_columns_relevant[t_cols].any(axis=1)\n",
    "berkeley_columns_relevant.loc[:, 'LGB+'] = berkeley_columns_relevant[lgb_cols].any(axis=1)\n",
    "berkeley_columns_relevant.loc[:, 'Disabled'] = berkeley_columns_relevant[disability_cols].any(axis=1)\n",
    "berkeley_columns_relevant.loc[:, 'Immigrant'] = berkeley_columns_relevant[immigrant_cols].any(axis=1)\n",
    "\n",
    "\n",
    "berkeley_columns_relevant = berkeley_columns_relevant.copy()\n",
    "berkeley_columns_relevant.rename(columns={'target_race_asian': 'Asian', 'target_race_black': 'Black', 'target_race_latinx': 'Latinx', \n",
    "'target_race_middle_eastern': 'Middle Eastern', 'target_race_pacific_islander': 'Pacific Islander', 'target_race_white': 'White', \n",
    "'target_religion_atheist': 'Atheist', 'target_religion_buddhist': 'Buddhist', 'target_religion_christian':'Christian','target_religion_hindu': 'Hindu', \n",
    "'target_religion_jewish': 'Jewish','target_religion_mormon': 'Mormon',\n",
    "'target_religion_muslim': 'Muslim', 'target_religion_other': 'Other Religion', 'target_gender_men': 'Men', \n",
    "'target_gender_non_binary': 'Non-Binary', 'target_gender_women': 'Women', 'target_race_native_american': 'Native American',\n",
    "}, inplace=True)\n",
    "\n",
    "berkeley_compressed = berkeley_columns_relevant.drop(lgb_cols + t_cols + disability_cols + immigrant_cols, axis=1)\n",
    "\n",
    "berkeley_compressed['hatespeech'] = berkeley_compressed['hatespeech'].apply(lambda x: 1 if x > 0 else 0)\n",
    "berkeley_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Asian</th>\n",
       "      <th>Black</th>\n",
       "      <th>Latinx</th>\n",
       "      <th>Middle Eastern</th>\n",
       "      <th>Native American</th>\n",
       "      <th>Pacific Islander</th>\n",
       "      <th>White</th>\n",
       "      <th>Atheist</th>\n",
       "      <th>Buddhist</th>\n",
       "      <th>Christianity</th>\n",
       "      <th>Hindu</th>\n",
       "      <th>Jewish</th>\n",
       "      <th>Muslim</th>\n",
       "      <th>Other Religion</th>\n",
       "      <th>Men</th>\n",
       "      <th>Non-Binary</th>\n",
       "      <th>Women</th>\n",
       "      <th>LGBTQ+</th>\n",
       "      <th>Disabled</th>\n",
       "      <th>Immigrant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Nonhate</th>\n",
       "      <td>3374</td>\n",
       "      <td>9200</td>\n",
       "      <td>4127</td>\n",
       "      <td>4986</td>\n",
       "      <td>2140</td>\n",
       "      <td>1662</td>\n",
       "      <td>6105</td>\n",
       "      <td>699</td>\n",
       "      <td>527</td>\n",
       "      <td>5974</td>\n",
       "      <td>943</td>\n",
       "      <td>2527</td>\n",
       "      <td>7704</td>\n",
       "      <td>1729</td>\n",
       "      <td>6991</td>\n",
       "      <td>1632</td>\n",
       "      <td>17196</td>\n",
       "      <td>17462</td>\n",
       "      <td>1351</td>\n",
       "      <td>5607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hate</th>\n",
       "      <td>3651</td>\n",
       "      <td>13699</td>\n",
       "      <td>4370</td>\n",
       "      <td>4464</td>\n",
       "      <td>679</td>\n",
       "      <td>696</td>\n",
       "      <td>3692</td>\n",
       "      <td>254</td>\n",
       "      <td>202</td>\n",
       "      <td>1236</td>\n",
       "      <td>342</td>\n",
       "      <td>4397</td>\n",
       "      <td>4805</td>\n",
       "      <td>599</td>\n",
       "      <td>3038</td>\n",
       "      <td>484</td>\n",
       "      <td>10693</td>\n",
       "      <td>9455</td>\n",
       "      <td>2365</td>\n",
       "      <td>5586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Asian  Black Latinx Middle Eastern Native American Pacific Islander  \\\n",
       "Nonhate  3374   9200   4127           4986            2140             1662   \n",
       "Hate     3651  13699   4370           4464             679              696   \n",
       "\n",
       "        White Atheist Buddhist Christianity Hindu Jewish Muslim  \\\n",
       "Nonhate  6105     699      527         5974   943   2527   7704   \n",
       "Hate     3692     254      202         1236   342   4397   4805   \n",
       "\n",
       "        Other Religion   Men Non-Binary  Women LGBTQ+ Disabled Immigrant  \n",
       "Nonhate           1729  6991       1632  17196  17462     1351      5607  \n",
       "Hate               599  3038        484  10693   9455     2365      5586  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns_to_sum = ['Asian', 'Black', 'Latinx', 'Middle Eastern', 'Native American', 'Pacific Islander', 'White', 'Atheist', 'Buddhist', 'Christian','Hindu', 'Jewish', 'Muslim', 'Mormon', 'Other Religion', 'Men', 'Non-Binary', 'Women', 'LGB+','Transgender', 'Disabled', 'Immigrant']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "hate_counts = berkeley_compressed[columns_to_sum].where(berkeley_compressed['hatespeech'] == 1).sum().to_frame().T\n",
    "nonhate_counts = berkeley_compressed[columns_to_sum].where(berkeley_compressed['hatespeech'] == 0).sum().to_frame().T\n",
    "\n",
    "target_counts = pd.concat([nonhate_counts, hate_counts], ignore_index=True)\n",
    "target_counts.index = ['Nonhate', 'Hate']\n",
    "\n",
    "\n",
    "target_counts.T.plot.bar(stacked=True)\n",
    "target_counts.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arabic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Class</th>\n",
       "      <th>english</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>الوزير جبران باسيل تاج راسك يا جربان ممنوع بعد...</td>\n",
       "      <td>abusive</td>\n",
       "      <td>Minister Gebran Bassil, crown your head, O Sco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>صديقي انت ابن جامعه اللعبه اكبر من داعش اللعبه...</td>\n",
       "      <td>normal</td>\n",
       "      <td>My friend, you are a university student. The g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>و مصلحة لبنان تبدأ باستخراج النفط و الغاز لوقف...</td>\n",
       "      <td>normal</td>\n",
       "      <td>Lebanon’s interest begins with extracting oil ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>وليد جنبلاط كاتب الحكمة يا قذر</td>\n",
       "      <td>abusive</td>\n",
       "      <td>Walid Jumblatt, writer of wisdom, filthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>شو بتلبقلك كلمة خنزير بتجي مفصله على قياسك وشكلك</td>\n",
       "      <td>abusive</td>\n",
       "      <td>What do you think about the word “pig” that is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>أمير المليشيا مش خائن</td>\n",
       "      <td>normal</td>\n",
       "      <td>The militia prince is not a traitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5842</th>\n",
       "      <td>صدقت يناسبك جدا جدا</td>\n",
       "      <td>normal</td>\n",
       "      <td>You are right, it suits you very well</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5843</th>\n",
       "      <td>لبخليني حب باسيل شغلتين</td>\n",
       "      <td>normal</td>\n",
       "      <td>Basil's love leaves me with two things</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5844</th>\n",
       "      <td>ليش تيوس الإمارت والسعودية مع السوريين واليمني...</td>\n",
       "      <td>hate</td>\n",
       "      <td>Why are the goats of the Emirates and Saudi Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5845</th>\n",
       "      <td>مش دفاعاً عن جبران باسيل بس كان لازم تحط المقط...</td>\n",
       "      <td>normal</td>\n",
       "      <td>Not in defense of Gebran Bassil, but you shoul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5846 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Tweet    Class  \\\n",
       "0     الوزير جبران باسيل تاج راسك يا جربان ممنوع بعد...  abusive   \n",
       "1     صديقي انت ابن جامعه اللعبه اكبر من داعش اللعبه...   normal   \n",
       "2     و مصلحة لبنان تبدأ باستخراج النفط و الغاز لوقف...   normal   \n",
       "3                        وليد جنبلاط كاتب الحكمة يا قذر  abusive   \n",
       "4      شو بتلبقلك كلمة خنزير بتجي مفصله على قياسك وشكلك  abusive   \n",
       "...                                                 ...      ...   \n",
       "5841                              أمير المليشيا مش خائن   normal   \n",
       "5842                                صدقت يناسبك جدا جدا   normal   \n",
       "5843                            لبخليني حب باسيل شغلتين   normal   \n",
       "5844  ليش تيوس الإمارت والسعودية مع السوريين واليمني...     hate   \n",
       "5845  مش دفاعاً عن جبران باسيل بس كان لازم تحط المقط...   normal   \n",
       "\n",
       "                                                english  \n",
       "0     Minister Gebran Bassil, crown your head, O Sco...  \n",
       "1     My friend, you are a university student. The g...  \n",
       "2     Lebanon’s interest begins with extracting oil ...  \n",
       "3              Walid Jumblatt, writer of wisdom, filthy  \n",
       "4     What do you think about the word “pig” that is...  \n",
       "...                                                 ...  \n",
       "5841                The militia prince is not a traitor  \n",
       "5842              You are right, it suits you very well  \n",
       "5843             Basil's love leaves me with two things  \n",
       "5844  Why are the goats of the Emirates and Saudi Ar...  \n",
       "5845  Not in defense of Gebran Bassil, but you shoul...  \n",
       "\n",
       "[5846 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_data = pd.read_csv('levantine_arabic.csv',sep = \",\")\n",
    "arabic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## german data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_data = pd.read_csv('german.csv',sep = \",\")\n",
    "german_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## korean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_data = pd.read_csv('korean.csv',sep = \",\")\n",
    "korean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_hate_counts = korean_train['hate'].value_counts()\n",
    "korean_hate_counts.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm-14064/ipykernel_76594/3649219314.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  shortened_berkeley[\"labels\"] = shortened_berkeley[\"labels\"].apply(lambda x: np.array(x, dtype=np.int64))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bc4bbac0e64f1881ef59e50d416ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2da5082346cd4cd0b4c4c0848500c651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b832952f8ef4309b926279db4a1481b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/122000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d7c941e3df4ad69e8e09ffa82207cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/13556 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoConfig\n",
    "\n",
    "#berkeley_compressed = berkeley_compressed.rename(columns={'hatespeech': 'label'})\n",
    "berkeley_compressed[demographic_labels] = berkeley_compressed[demographic_labels].astype(int)\n",
    "berkeley_compressed['labels'] = berkeley_compressed[demographic_labels].astype(float).values.tolist()\n",
    "train_test = Dataset.from_pandas(berkeley_compressed).train_test_split(test_size=0.1)\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "shortened_berkeley = berkeley_compressed.head(100)\n",
    "shortened_berkeley[\"labels\"] = shortened_berkeley[\"labels\"].apply(lambda x: np.array(x, dtype=np.int64))\n",
    "shortened_berkeley = shortened_berkeley[[\"text\", \"labels\"]]\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "features = Features({\n",
    "    \"text\": Value(\"string\"),\n",
    "    \"labels\": Sequence(Value(\"float32\"))  \n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "shortened_train_test = Dataset.from_pandas(berkeley_compressed).train_test_split(test_size=0.1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def preprocess(examples):\n",
    "    tokenized = tokenizer(examples[\"text\"], truncation=True) \n",
    "    tokenized[\"labels\"] = examples[\"labels\"]\n",
    "    return tokenized\n",
    "\n",
    "tokenized_data = train_test.map(preprocess, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "shortened_tokenized_data = shortened_train_test.map(preprocess, batched = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm-14064/ipykernel_76594/2049454021.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  shortened_berkeley[\"labels\"] = shortened_berkeley[\"labels\"].apply(lambda x: np.array(x, dtype=np.int64))\n"
     ]
    }
   ],
   "source": [
    "shortened_berkeley = berkeley_compressed.head(100)\n",
    "shortened_berkeley[\"labels\"] = shortened_berkeley[\"labels\"].apply(lambda x: np.array(x, dtype=np.int64))\n",
    "shortened_berkeley = shortened_berkeley[[\"text\", \"labels\"]]\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "features = Features({\n",
    "    \"text\": Value(\"string\"),\n",
    "    \"labels\": Sequence(Value(\"float32\"))  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/scratch/slurm-14064/ipykernel_76594/1718452880.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38125' max='38125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38125/38125 51:02, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.093100</td>\n",
       "      <td>0.092587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.089761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.078600</td>\n",
       "      <td>0.090460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.093913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.065400</td>\n",
       "      <td>0.097674</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=38125, training_loss=0.0879882236918465, metrics={'train_runtime': 3062.4352, 'train_samples_per_second': 199.188, 'train_steps_per_second': 12.449, 'total_flos': 1.5802946869776e+16, 'train_loss': 0.0879882236918465, 'epoch': 5.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=20, problem_type=\"multi_label_classification\")\n",
    "\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "    return {\n",
    "        \"f1_micro\": f1.compute(predictions=preds, references=labels, average=\"micro\")[\"f1\"],\n",
    "        \"precision_micro\": precision.compute(predictions=preds, references=labels, average=\"micro\")[\"precision\"],\n",
    "        \"recall_micro\": recall.compute(predictions=preds, references=labels, average=\"micro\")[\"recall\"],\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"hatespeech_classifier\",\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    warmup_ratio=0.1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"],\n",
    "    eval_dataset=tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n",
    "#torch.mps.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#results = trainer.evaluate(tokenized_data['test'])\n",
    "#pd.DataFrame(results, index=['Fine-tuned DistilBERT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classifying text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text):\n",
    "    predict_input = tokenizer.encode(text,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors=\"tf\")\n",
    "    output = model(predict_input)[0]\n",
    "    prediction_value = tf.argmax(output, axis=1).numpy()[0]\n",
    "    return prediction_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_data = unlabelled_df[‘data’].to_list()\n",
    "len(unlabelled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_predictions = []\n",
    "for data in unlabelled_data:\n",
    "    unlabelled_predictions.append(predict_category(data))\n",
    "    prediction_df = pd.DataFrame({\n",
    "        \"data\": unlabelled_data,\n",
    "        \"labels\": unlabelled_predictions,\n",
    "    })\n",
    "prediction_df.to_csv(\"model_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5846"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arabic_en = arabic_data['english'].to_list()\n",
    "len(arabic_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m arabic_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m arabic_en:\n\u001b[0;32m----> 3\u001b[0m     arabic_predictions\u001b[38;5;241m.\u001b[39mappend(predict_category(data))\n\u001b[1;32m      4\u001b[0m     arabic_prediction_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m: arabic_en,\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: arabic_predictions,\n\u001b[1;32m      7\u001b[0m     })\n\u001b[1;32m      8\u001b[0m prediction_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_prediction.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[43], line 6\u001b[0m, in \u001b[0;36mpredict_category\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_category\u001b[39m(text):\n\u001b[1;32m      2\u001b[0m     predict_input \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(text,\n\u001b[1;32m      3\u001b[0m     truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m     )\n\u001b[0;32m----> 6\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(predict_input)\n\u001b[1;32m      7\u001b[0m     prediction_value \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(output, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m prediction_value\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:977\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    975\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 977\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistilbert(\n\u001b[1;32m    978\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    979\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    980\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    981\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m    982\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    983\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    984\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    985\u001b[0m )\n\u001b[1;32m    986\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    987\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:771\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m    772\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:5153\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   5150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   5152\u001b[0m \u001b[38;5;66;03m# Check only the first and last input IDs to reduce overhead.\u001b[39;00m\n\u001b[0;32m-> 5153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01min\u001b[39;00m input_ids[:, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m   5154\u001b[0m     warn_string \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   5155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/troubleshooting\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#incorrect-output-when-padding-tokens-arent-masked.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5158\u001b[0m     )\n\u001b[1;32m   5160\u001b[0m     \u001b[38;5;66;03m# If the pad token is equal to either BOS, EOS, or SEP, we do not know whether the user should use an\u001b[39;00m\n\u001b[1;32m   5161\u001b[0m     \u001b[38;5;66;03m# attention_mask or not. In this case, we should still show a warning because this is a rare case.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "arabic_predictions = []\n",
    "for data in arabic_en:\n",
    "    arabic_predictions.append(predict_category(data))\n",
    "    arabic_prediction_df = pd.DataFrame({\n",
    "        \"english\": arabic_en,\n",
    "        \"labels\": arabic_predictions,\n",
    "    })\n",
    "prediction_df.to_csv(\"model_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_list = german[‘english’].to_list()\n",
    "len(korean_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## korean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "korean_list = korean[‘english’].to_list()\n",
    "len(korean_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# practice code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm-14064/ipykernel_76594/40129132.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='848' max='848' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [848/848 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)},\nInput predictions: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 1 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 1 0 ... 1 0 0]\n [0 0 0 ... 0 0 0]],\nInput references: [[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 1. 0. 0.]\n [0. 0. 0. ... 1. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 1. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#torch.mps.empty_cache()\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#trainer.train()\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(shortened_tokenized_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#short_results = []\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#for item in shortened_tokenized_data['test']:\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#result = trainer.evaluate(item)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#print(result)\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#short_results.append(result)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(short_results, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFine-tuned DistilBERT\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:4105\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4102\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4104\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4105\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[1;32m   4106\u001b[0m     eval_dataloader,\n\u001b[1;32m   4107\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4108\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[1;32m   4109\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[1;32m   4110\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4111\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[1;32m   4112\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[1;32m   4113\u001b[0m )\n\u001b[1;32m   4115\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:4394\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4392\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4393\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 4394\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   4395\u001b[0m         EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39meval_set_kwargs)\n\u001b[1;32m   4396\u001b[0m     )\n\u001b[1;32m   4397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4398\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m     11\u001b[0m probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(torch\u001b[38;5;241m.\u001b[39mtensor(logits))\n\u001b[1;32m     12\u001b[0m preds \u001b[38;5;241m=\u001b[39m (probs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_micro\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_micro\u001b[39m\u001b[38;5;124m\"\u001b[39m: precision\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_micro\u001b[39m\u001b[38;5;124m\"\u001b[39m: recall\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m }\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/evaluate/module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/evaluate/module.py:546\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m     )\n\u001b[0;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)},\nInput predictions: [[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 1 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 1 0 ... 1 0 0]\n [0 0 0 ... 0 0 0]],\nInput references: [[0. 0. 0. ... 0. 0. 0.]\n [0. 0. 0. ... 1. 0. 0.]\n [0. 0. 0. ... 1. 0. 0.]\n ...\n [0. 0. 0. ... 0. 0. 0.]\n [0. 1. 0. ... 1. 0. 0.]\n [0. 1. 0. ... 0. 0. 0.]]"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=shortened_tokenized_data[\"train\"],\n",
    "    eval_dataset=shortened_tokenized_data[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "#torch.mps.empty_cache()\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "#result = trainer.evaluate(shortened_tokenized_data['test'])\n",
    "#short_results = []\n",
    "#for item in shortened_tokenized_data['test']:\n",
    "    #result = trainer.evaluate(item)\n",
    "    #print(result)\n",
    "    #short_results.append(result)\n",
    "    \n",
    "pd.DataFrame(short_results, index=['Fine-tuned DistilBERT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m shortened_berkeley \u001b[38;5;241m=\u001b[39m berkeley_compressed\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m shortened_berkeley[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m shortened_berkeley[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64))\n\u001b[1;32m      3\u001b[0m shortened_berkeley \u001b[38;5;241m=\u001b[39m shortened_berkeley[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages/pandas/core/series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4640\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[1;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4759\u001b[0m         func,\n\u001b[1;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[1;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[1;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[0;32m/ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages/pandas/core/apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[0;32m/ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages/pandas/core/apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[1;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[1;32m   1291\u001b[0m )\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[0;32m/ihome/crc/install/jupyterhub/hub4.0.2/lib/python3.11/site-packages/pandas/core/algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[1;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1818\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m shortened_berkeley \u001b[38;5;241m=\u001b[39m berkeley_compressed\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m shortened_berkeley[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m shortened_berkeley[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39marray(x, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64))\n\u001b[1;32m      3\u001b[0m shortened_berkeley \u001b[38;5;241m=\u001b[39m shortened_berkeley[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m      4\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "shortened_berkeley = berkeley_compressed.head(100)\n",
    "shortened_berkeley[\"labels\"] = shortened_berkeley[\"labels\"].apply(lambda x: np.array(x, dtype=np.int64))\n",
    "shortened_berkeley = shortened_berkeley[[\"text\", \"labels\"]]\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "features = Features({\n",
    "    \"text\": Value(\"string\"),\n",
    "    \"labels\": Sequence(Value(\"float32\"))  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_short = Dataset.from_pandas(shortened_berkeley, features=features).train_test_split(test_size=0.1)\n",
    "tokenizer_short = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(examples):\n",
    "    tokenized = tokenizer_short(examples[\"text\"], truncation=True, padding=True) \n",
    "\n",
    "    labels = examples[\"labels\"]\n",
    "    labels = np.array(labels).astype(np.float32)\n",
    "    \n",
    "    tokenized[\"labels\"] = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c817542496490e8b6ad1f0203e42cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbced0fe37ab417daa02126032d032e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data_short = train_test_short.map(preprocess, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Question: These 4 broads who criticize America, what country did they flee to get here? And now they want to make OUR America like THEIR former HELL HOLE. I don't think so!!!!!!!!!!  Let them explain their GRATITUDE for letting them in OUR country.\", 'labels': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], 'input_ids': [101, 3160, 1024, 2122, 1018, 5041, 2015, 2040, 6232, 4697, 2637, 1010, 2054, 2406, 2106, 2027, 10574, 2000, 2131, 2182, 1029, 1998, 2085, 2027, 2215, 2000, 2191, 2256, 2637, 2066, 2037, 2280, 3109, 4920, 1012, 1045, 2123, 1005, 1056, 2228, 2061, 999, 999, 999, 999, 999, 999, 999, 999, 999, 999, 2292, 2068, 4863, 2037, 15531, 2005, 5599, 2068, 1999, 2256, 2406, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_data_short[\"train\"][0])\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=20,\n",
    "    problem_type=\"multi_label_classification\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_short = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f1 = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    logits = torch.tensor(logits, dtype=torch.float).cpu()  \n",
    "    labels = torch.tensor(labels, dtype=torch.float).cpu()\n",
    "\n",
    "    probs = torch.sigmoid(torch.tensor(logits))\n",
    "    preds = (probs > 0.5).int().numpy()\n",
    "    labels = np.array(labels)\n",
    "\n",
    "  \n",
    "    return {\n",
    "        \"f1_micro\": f1.compute(predictions=preds, references=labels, average=\"micro\")[\"f1\"],\n",
    "        \"precision_micro\": precision.compute(predictions=preds, references=labels, average=\"micro\")[\"precision\"],\n",
    "        \"recall_micro\": recall.compute(predictions=preds, references=labels, average=\"micro\")[\"recall\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm-14062/ipykernel_78845/2188472962.py:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_short = Trainer(\n",
      "Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]]), 'input_ids': tensor([[ 101, 9122, 3393,  ...,    0,    0,    0],\n",
      "        [ 101, 2138, 2009,  ...,    0,    0,    0],\n",
      "        [ 101, 9252, 2465,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 4067, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 6616, 3398,  ...,    0,    0,    0],\n",
      "        [ 101, 1000, 2073,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.448916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.4757084051767985, metrics={'train_runtime': 20.6264, 'train_samples_per_second': 4.363, 'train_steps_per_second': 0.291, 'total_flos': 2585496315600.0, 'train_loss': 0.4757084051767985, 'epoch': 1.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"hatespeech_classifier\",\n",
    "    learning_rate=4e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    warmup_ratio=0.1\n",
    ")\n",
    "\n",
    "trainer_short = Trainer(\n",
    "    model=model_short,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data_short[\"train\"],\n",
    "    eval_dataset=tokenized_data_short[\"test\"],\n",
    "    tokenizer=tokenizer_short,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "for batch in trainer_short.get_train_dataloader():\n",
    "    print(batch)\n",
    "    break\n",
    "trainer_short.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]]), 'input_ids': tensor([[ 101, 9122, 3393,  ...,    0,    0,    0],\n",
      "        [ 101, 2138, 2009,  ...,    0,    0,    0],\n",
      "        [ 101, 9252, 2465,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 4067, 2017,  ...,    0,    0,    0],\n",
      "        [ 101, 6616, 3398,  ...,    0,    0,    0],\n",
      "        [ 101, 1000, 2073,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:19, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.411055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=0.43312708536783856, metrics={'train_runtime': 21.1811, 'train_samples_per_second': 4.249, 'train_steps_per_second': 0.283, 'total_flos': 2585496315600.0, 'train_loss': 0.43312708536783856, 'epoch': 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in trainer_short.get_train_dataloader():\n",
    "    print(batch)\n",
    "    break\n",
    "trainer_short.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAccuracy is the proportion of correct predictions among the total number of cases processed. It can be computed with:\\nAccuracy = (TP + TN) / (TP + TN + FP + FN)\\n Where:\\nTP: True positive\\nTN: True negative\\nFP: False positive\\nFN: False negative\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for model_inputs, gold_standards in evaluation_dataset:\n",
    " #   predictions = model(model_inputs)\n",
    "#  metric.add_batch(references=gold_standards, predictions=predictions)\n",
    "#metric.compute()\n",
    "\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "#accuracy.description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/slurm-14062/ipykernel_78845/1874909447.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  probs = torch.sigmoid(torch.tensor(logits))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)},\nInput predictions: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]],\nInput references: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m trainer_short\u001b[38;5;241m.\u001b[39mevaluate(tokenized_data_short[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      2\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(results, index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFine-tuned DistilBERT\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:4105\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4102\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4104\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4105\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[1;32m   4106\u001b[0m     eval_dataloader,\n\u001b[1;32m   4107\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4108\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[1;32m   4109\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[1;32m   4110\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4111\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[1;32m   4112\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[1;32m   4113\u001b[0m )\n\u001b[1;32m   4115\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/transformers/trainer.py:4394\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4392\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4393\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 4394\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   4395\u001b[0m         EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39meval_set_kwargs)\n\u001b[1;32m   4396\u001b[0m     )\n\u001b[1;32m   4397\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4398\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m      7\u001b[0m preds \u001b[38;5;241m=\u001b[39m (probs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(labels)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1_micro\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision_micro\u001b[39m\u001b[38;5;124m\"\u001b[39m: precision\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall_micro\u001b[39m\u001b[38;5;124m\"\u001b[39m: recall\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     15\u001b[0m }\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/evaluate/module.py:455\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 455\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_batch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/evaluate/module.py:546\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredictions and/or references don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match the expected format.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    543\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput predictions: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput references: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msummarize_if_long_list(references)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m     )\n\u001b[0;32m--> 546\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Predictions and/or references don't match the expected format.\nExpected format: {'predictions': Value(dtype='int32', id=None), 'references': Value(dtype='int32', id=None)},\nInput predictions: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]],\nInput references: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]]"
     ]
    }
   ],
   "source": [
    "#for model_inputs, gold_standards in evaluation_dataset:\n",
    " #   predictions = model(model_inputs)\n",
    "#  metric.add_batch(references=gold_standards, predictions=predictions)\n",
    "#metric.compute()\n",
    "\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "\n",
    "results = trainer_short.evaluate(tokenized_data_short['test'])\n",
    "pd.DataFrame(results, index=['Fine-tuned DistilBERT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
